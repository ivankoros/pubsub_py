{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-05T17:21:32.132055Z",
     "start_time": "2023-04-05T17:21:32.118058Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Ivan\n",
      "[nltk_data]     Korostenskij\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk.corpus\n",
    "import spacy\n",
    "from pprint import pprint\n",
    "from twilio_app import all_sandwiches\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-05T17:21:32.898056Z",
     "start_time": "2023-04-05T17:21:32.862054Z"
    }
   },
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "\n",
    "    # Lowercase because we don't want to treat \"Hello\" and \"hello\" differently\n",
    "    text = text.lower()\n",
    "\n",
    "    # Remove unicode with regex because it's irrelevant for our task and might cause problems\n",
    "    text = re.sub(r\"(@\\[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)|^rt|http.+?\", \"\", text)\n",
    "\n",
    "    # Remove stopwords\n",
    "    \"\"\"\n",
    "    Stopwords are words that are so common that they don't carry much meaning.\n",
    "    For example, \"the\", \"a\", \"is\", \"are\", etc.\n",
    "\n",
    "    \"\"\"\n",
    "    stop = stopwords.words('english')\n",
    "    text = \" \".join([word for word in text.split() if word not in stop])\n",
    "\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-05T17:25:02.741949Z",
     "start_time": "2023-04-05T17:25:00.544948Z"
    }
   },
   "outputs": [],
   "source": [
    "default_toppings = {\n",
    "    \"lettuce\": True,\n",
    "    \"tomato\": True,\n",
    "    \"onions\": True,\n",
    "    \"pickles\": True,\n",
    "    \"banana_peppers\": False,\n",
    "    \"black_olives\": False,\n",
    "    \"green_peppers\": False,\n",
    "    \"jalapeno_peppers\": False,\n",
    "    \"cucumbers\": False,\n",
    "    \"spinach\": False,\n",
    "    \"salt\": False,\n",
    "    \"pepper\": False,\n",
    "    \"oregano\": False,\n",
    "    \"oil_vinegar\": False,\n",
    "    \"mustard\": False,\n",
    "    \"mayonnaise\": True,\n",
    "    \"ranch\": False,\n",
    "    \"chipotle_sauce\": False,\n",
    "    \"honey_mustard\": False,\n",
    "    \"spicy_mustard\": False,\n",
    "    \"sub_dressing\": False,\n",
    "    \"pepperhouse_gourmaise\": False,\n",
    "    \"deli_sub_sauce\": False,\n",
    "    \"vegan_ranch_dressing\": False,\n",
    "    \"buttermilk_ranch\": False,\n",
    "}\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-05T17:48:11.847816Z",
     "end_time": "2023-04-05T17:48:11.883815Z"
    }
   },
   "outputs": [],
   "source": [
    "def parse_sandwich_order(input_string):\n",
    "    # Turn the input string into a spaCy document\n",
    "    doc = nlp(input_string.lower())\n",
    "\n",
    "    # Initialize the sandwich name and toppings dictionary\n",
    "    sandwich_name = \"\"\n",
    "    toppings = {}\n",
    "\n",
    "    # Loop through the tokens in the input string\n",
    "    for token in doc:\n",
    "        # If the token is a noun or adjective that describes a sandwich, add it to the sandwich order\n",
    "        if token.pos_ in [\"NOUN\", \"PROPN\", \"ADJ\"] and (\"sub\" in token.text or \"sandwich\" in token.text)\\\n",
    "                or token.text in [s_wish.lower() for s_wish in all_sandwiches]:\n",
    "            sandwich_name += all_sandwiches[token.text] + \" \"\n",
    "\n",
    "        # If the token is a negative phrase, remove the corresponding topping from the toppings dictionary\n",
    "        elif token.dep_ == \"neg\" and token.head.pos_ == \"NOUN\" and token.head.text in [toppin.lower() for toppin in default_toppings.items()]:\n",
    "            default_toppings[token.head.text] = False\n",
    "\n",
    "        # If the token is a topping, add it to the toppings dictionary\n",
    "        elif token.pos_ == \"NOUN\" and token.text.lower() in default_toppings:\n",
    "            default_toppings[token.text] = True\n",
    "\n",
    "        # most_likely_sandwich = []\n",
    "        # sub_text_length = len(sandwich_name)\n",
    "        # if sandwich_name != \"\":\n",
    "        #     for sub in all_sandwiches:\n",
    "        #         if sandwich_name in sub:\n",
    "        #             most_likely_sandwich.append(len(sub)/sub_text_length)\n",
    "        #     most_likely_sandwich = max(most_likely_sandwich)\n",
    "        #     print(f\"Most likely sandwich: {most_likely_sandwich}\")\n",
    "\n",
    "    return sandwich_name.strip(), default_toppings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-05T17:07:31.718630Z",
     "start_time": "2023-04-05T17:07:31.686632Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let me get a hot Italian, no mayo\n",
      "Sandwich order:\n",
      "''\n",
      "Toppings:\n",
      "{'banana_peppers': False,\n",
      " 'black_olives': False,\n",
      " 'buttermilk_ranch': False,\n",
      " 'chipotle_sauce': False,\n",
      " 'cucumbers': False,\n",
      " 'deli_sub_sauce': False,\n",
      " 'green_peppers': False,\n",
      " 'honey_mustard': False,\n",
      " 'jalapeno_peppers': False,\n",
      " 'lettuce': True,\n",
      " 'mayonnaise': True,\n",
      " 'mustard': False,\n",
      " 'oil_vinegar': False,\n",
      " 'onions': True,\n",
      " 'oregano': False,\n",
      " 'pepper': False,\n",
      " 'pepperhouse_gourmaise': False,\n",
      " 'pickles': True,\n",
      " 'ranch': False,\n",
      " 'salt': False,\n",
      " 'spicy_mustard': False,\n",
      " 'spinach': False,\n",
      " 'sub_dressing': False,\n",
      " 'tomato': True,\n",
      " 'vegan_ranch_dressing': False}\n"
     ]
    }
   ],
   "source": [
    "# Test the function with a sample input\n",
    "sub_order_text = \"Let me get a hot Italian, no mayo\"\n",
    "print(sub_order_text)\n",
    "# Clean the text\n",
    "#sub_order_text_cleaned = clean_text(sub_order_text)\n",
    "\n",
    "# Parse the sandwich order and get the sandwich name and toppings\n",
    "sandwich_name, toppings = parse_sandwich_order(sub_order_text)\n",
    "print(\"Sandwich order:\")\n",
    "pprint(sandwich_name)\n",
    "print(\"Toppings:\")\n",
    "pprint(toppings)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
