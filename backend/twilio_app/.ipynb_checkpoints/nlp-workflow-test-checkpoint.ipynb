{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-05T17:21:32.132055Z",
     "start_time": "2023-04-05T17:21:32.118058Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Ivan\n",
      "[nltk_data]     Korostenskij\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk.corpus\n",
    "import spacy\n",
    "from pprint import pprint\n",
    "from twilio_app import all_sandwiches\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-05T17:21:32.898056Z",
     "start_time": "2023-04-05T17:21:32.862054Z"
    }
   },
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "\n",
    "    # Lowercase because we don't want to treat \"Hello\" and \"hello\" differently\n",
    "    text = text.lower()\n",
    "\n",
    "    # Remove unicode with regex because it's irrelevant for our task and might cause problems\n",
    "    text = re.sub(r\"(@\\[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)|^rt|http.+?\", \"\", text)\n",
    "\n",
    "    # Remove stopwords\n",
    "    \"\"\"\n",
    "    Stopwords are words that are so common that they don't carry much meaning.\n",
    "    For example, \"the\", \"a\", \"is\", \"are\", etc.\n",
    "\n",
    "    \"\"\"\n",
    "    stop = stopwords.words('english')\n",
    "    text = \" \".join([word for word in text.split() if word not in stop])\n",
    "\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-05T17:25:02.741949Z",
     "start_time": "2023-04-05T17:25:00.544948Z"
    }
   },
   "outputs": [],
   "source": [
    "default_toppings = {\n",
    "    \"lettuce\": True,\n",
    "    \"tomato\": True,\n",
    "    \"onions\": True,\n",
    "    \"pickles\": True,\n",
    "    \"banana_peppers\": False,\n",
    "    \"black_olives\": False,\n",
    "    \"green_peppers\": False,\n",
    "    \"jalapeno_peppers\": False,\n",
    "    \"cucumbers\": False,\n",
    "    \"spinach\": False,\n",
    "    \"salt\": False,\n",
    "    \"pepper\": False,\n",
    "    \"oregano\": False,\n",
    "    \"oil_vinegar\": False,\n",
    "    \"mustard\": False,\n",
    "    \"mayonnaise\": False,\n",
    "    \"ranch\": False,\n",
    "    \"chipotle_sauce\": False,\n",
    "    \"honey_mustard\": False,\n",
    "    \"spicy_mustard\": False,\n",
    "    \"sub_dressing\": False,\n",
    "    \"pepperhouse_gourmaise\": False,\n",
    "    \"deli_sub_sauce\": False,\n",
    "    \"vegan_ranch_dressing\": False,\n",
    "    \"buttermilk_ranch\": False,\n",
    "}\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-05T17:25:46.971170Z",
     "start_time": "2023-04-05T17:25:46.942169Z"
    }
   },
   "outputs": [],
   "source": [
    "def parse_sandwich_order(input_string):\n",
    "    # Turn the input string into a spaCy document\n",
    "    doc = nlp(input_string.lower())\n",
    "\n",
    "    # Initialize the sandwich name and toppings dictionary\n",
    "    sandwich_name = \"\"\n",
    "    toppings = {}\n",
    "\n",
    "    # Loop through the tokens in the input string\n",
    "    for token in doc:\n",
    "        # If the token is a noun or adjective that describes a sandwich, add it to the sandwich order\n",
    "        if token.pos_ in [\"NOUN\", \"PROPN\", \"ADJ\"] and (\"sub\" in token.text or \"sandwich\" in token.text)\\\n",
    "                or token.text in all_sandwiches:\n",
    "            sandwich_name += all_sandwiches[token.text] + \" \"\n",
    "        # If the token is a negative phrase, remove the corresponding topping from the toppings dictionary\n",
    "        elif token.dep_ == \"neg\" and token.head.pos_ == \"NOUN\" and token.head.text in toppings:\n",
    "            default_toppings[token.head.text] = False\n",
    "\n",
    "        # If the token is a topping, add it to the toppings dictionary\n",
    "        elif token.pos_ == \"NOUN\" and token.text.lower() in default_toppings.items().lower():\n",
    "            default_toppings[token.text] = True\n",
    "\n",
    "        # most_likely_sandwich = []\n",
    "        # sub_text_length = len(sandwich_name)\n",
    "        # if sandwich_name != \"\":\n",
    "        #     for sub in all_sandwiches:\n",
    "        #         if sandwich_name in sub:\n",
    "        #             most_likely_sandwich.append(len(sub)/sub_text_length)\n",
    "        #     most_likely_sandwich = max(most_likely_sandwich)\n",
    "        #     print(f\"Most likely sandwich: {most_likely_sandwich}\")\n",
    "\n",
    "    return sandwich_name.strip(), default_toppings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-05T17:07:31.718630Z",
     "start_time": "2023-04-05T17:07:31.686632Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let me get a hot Italian, no mayo\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'dict_items' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[33], line 8\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(sub_order_text)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Clean the text\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m#sub_order_text_cleaned = clean_text(sub_order_text)\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Parse the sandwich order and get the sandwich name and toppings\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m sandwich_name, toppings \u001b[38;5;241m=\u001b[39m \u001b[43mparse_sandwich_order\u001b[49m\u001b[43m(\u001b[49m\u001b[43msub_order_text\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSandwich order:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     10\u001b[0m pprint(sandwich_name)\n",
      "Cell \u001b[1;32mIn[32], line 20\u001b[0m, in \u001b[0;36mparse_sandwich_order\u001b[1;34m(input_string)\u001b[0m\n\u001b[0;32m     17\u001b[0m     default_toppings[token\u001b[38;5;241m.\u001b[39mhead\u001b[38;5;241m.\u001b[39mtext] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# If the token is a topping, add it to the toppings dictionary\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m token\u001b[38;5;241m.\u001b[39mpos_ \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOUN\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m token\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;129;01min\u001b[39;00m \u001b[43mdefault_toppings\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlower\u001b[49m():\n\u001b[0;32m     21\u001b[0m     default_toppings[token\u001b[38;5;241m.\u001b[39mtext] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# most_likely_sandwich = []\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# sub_text_length = len(sandwich_name)\u001b[39;00m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# if sandwich_name != \"\":\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m#     most_likely_sandwich = max(most_likely_sandwich)\u001b[39;00m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m#     print(f\"Most likely sandwich: {most_likely_sandwich}\")\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'dict_items' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "# Test the function with a sample input\n",
    "sub_order_text = \"Let me get a hot Italian, no mayo\"\n",
    "print(sub_order_text)\n",
    "# Clean the text\n",
    "#sub_order_text_cleaned = clean_text(sub_order_text)\n",
    "\n",
    "# Parse the sandwich order and get the sandwich name and toppings\n",
    "sandwich_name, toppings = parse_sandwich_order(sub_order_text)\n",
    "print(\"Sandwich order:\")\n",
    "pprint(sandwich_name)\n",
    "print(\"Toppings:\")\n",
    "pprint(toppings)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
